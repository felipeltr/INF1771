{"paragraphs":[{"text":"%dep\n\nz.load(\"/Users/ferocha/Downloads/gensim-doc2vec-spark-master/ddoc2vecf.py\")\n// z.load(\"avulanov:scalable-deeplearning:1.0.0\")","dateUpdated":"2016-12-06T10:30:30-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480634837216_-12602622","id":"20161201-212717_1743513429","result":{"code":"SUCCESS","type":"TEXT","msg":"DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@17b9f2aa\n"},"dateCreated":"2016-12-01T09:27:17-0200","dateStarted":"2016-12-03T15:36:50-0200","dateFinished":"2016-12-03T15:36:55-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:301"},{"title":"Data load","text":"%spark.pyspark\n\nfrom pyspark.sql.functions import *\nimport os\n\nif os.path.exists(\"/Users/ferocha/Desktop/ia\"):\n    trainDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/ia/train_parquet\").cache()\n    testDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/ia/test_parquet\").cache()\n    \nelse:\n    posDf = spark.read.text(\"/Users/ferocha/Downloads/movie_review_dataset/part*/pos/*.txt\") \\\n        .withColumnRenamed(\"value\",\"content\").withColumn(\"label\",lit(1.0))\n    negDf = spark.read.text(\"/Users/ferocha/Downloads/movie_review_dataset/part*/neg/*.txt\") \\\n        .withColumnRenamed(\"value\",\"content\").withColumn(\"label\",lit(0.0))\n        \n    trainPosDf, testPosDf = posDf.randomSplit([0.9,0.1], seed=1234)\n    trainNegDf, testNegDf = negDf.randomSplit([0.9,0.1], seed=1234)\n    \n    trainDf = trainPosDf.union(trainNegDf).cache()\n    testDf = testPosDf.union(testNegDf).cache()\n\n    trainDf.write.parquet(\"file:///Users/ferocha/Desktop/ia/train_parquet\")\n    testDf.write.parquet(\"file:///Users/ferocha/Desktop/ia/test_parquet\")\n","dateUpdated":"2016-12-06T10:35:07-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480442986369_-1038057595","id":"20161129-160946_39271133","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-29T04:09:46-0200","dateStarted":"2016-12-06T10:35:07-0200","dateFinished":"2016-12-06T10:35:57-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:302"},{"text":"%spark.pyspark\n\ntestDf.count()","dateUpdated":"2016-11-30T02:33:37-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480523593708_43394436","id":"20161130-143313_1676212009","result":{"code":"SUCCESS","type":"TEXT","msg":"4784\n"},"dateCreated":"2016-11-30T02:33:13-0200","dateStarted":"2016-11-30T02:33:37-0200","dateFinished":"2016-11-30T02:33:46-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:303"},{"text":"%md\n\n## Text Preprocessing and Feature Extraction","dateUpdated":"2016-12-01T02:13:27-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480565541857_-549627702","id":"20161201-021221_2076445419","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Text Preprocessing and Feature Extraction</h2>\n"},"dateCreated":"2016-12-01T02:12:21-0200","dateStarted":"2016-12-01T02:13:09-0200","dateFinished":"2016-12-01T02:13:09-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:304"},{"title":"Tokenize and remove stopwords","text":"%spark.pyspark\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\n\n#// We use nltk tokenizer for better results\nfrom nltk import word_tokenize\nimport re\n\ndef tok(sentence):\n    return [w for w in word_tokenize(sentence.replace(\"<br />\",\" \").replace(\"/\", \" / \") \\\n        .replace(\"'\",\" ' \").replace(\"\\\"\",\" \\\" \").replace(\"*\",\" \")) if re.search('\\w',w) != None]\n\ntokFunction = udf(tok,ArrayType(StringType()))\n\ntokTrainDf = trainDf.withColumn(\"words\",tokFunction(trainDf.content))\ntokTestDf = testDf.withColumn(\"words\",tokFunction(testDf.content))\n\nstopRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filteredWords\")\n\ntokTrainDf = stopRemover.transform(tokTrainDf).drop(\"words\").cache()\ntokTestDf = stopRemover.transform(tokTestDf).drop(\"words\").cache()\n\ntokTrainDf.show(10)","dateUpdated":"2016-12-06T10:35:13-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"filteredWords","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"filteredWords","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":false,"title":true,"lineNumbers":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480448154334_-487316118","id":"20161129-173554_2096425000","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+-----+--------------------+\n|             content|label|       filteredWords|\n+--------------------+-----+--------------------+\n|'The Merchant of ...|  1.0|[Merchant, Venice...|\n|(Some spoilers in...|  1.0|[spoilers, includ...|\n|*!!- SPOILERS - !...|  1.0|[SPOILERS, begin,...|\n|**Attention Spoil...|  1.0|[Attention, Spoil...|\n|As I expected wou...|  1.0|[expected, would,...|\n|As with all of An...|  1.0|[Angelopoulos, fi...|\n|Back in the mid/l...|  1.0|[Back, mid, late,...|\n|Beat a path to th...|  1.0|[Beat, path, impo...|\n|Berlin-born in 19...|  1.0|[Berlin-born, 194...|\n|By 1987 Hong Kong...|  1.0|[1987, Hong, Kong...|\n+--------------------+-----+--------------------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-29T05:35:54-0200","dateStarted":"2016-12-06T10:35:13-0200","dateFinished":"2016-12-06T10:36:16-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:305"},{"text":"%spark.pyspark\n\nfeaturedTrainDf = tokTrainDf\nfeaturedTestDf = tokTestDf","dateUpdated":"2016-12-06T10:35:17-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480547033563_422555542","id":"20161130-210353_90064210","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-30T09:03:53-0200","dateStarted":"2016-12-06T10:35:57-0200","dateFinished":"2016-12-06T10:36:16-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:306"},{"title":"Feature Extraction using TF_IDF","text":"%spark.pyspark\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nhashingTF = HashingTF(inputCol=\"filteredWords\", outputCol=\"rawFeatures\", numFeatures=4096)\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"tfidfFeatures\")\n\ntfIdfPipeline = Pipeline(stages=[hashingTF,idf])\nmodel = tfIdfPipeline.fit(featuredTrainDf)\n\nfeaturedTrainDf = model.transform(featuredTrainDf).drop(\"rawFeatures\").cache()\nfeaturedTestDf = model.transform(featuredTestDf).drop(\"rawFeatures\").cache()\n\nfeaturedTrainDf.show(10)\n","dateUpdated":"2016-12-06T11:09:38-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480443364730_1118979368","id":"20161129-161604_1270778657","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+-----+--------------------+--------------------+\n|             content|label|       filteredWords|       tfidfFeatures|\n+--------------------+-----+--------------------+--------------------+\n|'The Merchant of ...|  1.0|[Merchant, Venice...|(256,[0,1,3,4,6,7...|\n|(Some spoilers in...|  1.0|[spoilers, includ...|(256,[0,1,2,3,4,5...|\n|*!!- SPOILERS - !...|  1.0|[SPOILERS, begin,...|(256,[0,1,2,3,4,5...|\n|**Attention Spoil...|  1.0|[Attention, Spoil...|(256,[0,1,2,3,4,6...|\n|As I expected wou...|  1.0|[expected, would,...|(256,[0,1,2,3,4,5...|\n|As with all of An...|  1.0|[Angelopoulos, fi...|(256,[1,2,3,4,6,8...|\n|Back in the mid/l...|  1.0|[Back, mid, late,...|(256,[0,1,2,4,5,6...|\n|Beat a path to th...|  1.0|[Beat, path, impo...|(256,[1,4,5,6,7,8...|\n|Berlin-born in 19...|  1.0|[Berlin-born, 194...|(256,[0,2,3,4,5,6...|\n|By 1987 Hong Kong...|  1.0|[1987, Hong, Kong...|(256,[0,1,2,3,4,6...|\n+--------------------+-----+--------------------+--------------------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-29T04:16:04-0200","dateStarted":"2016-12-06T10:36:16-0200","dateFinished":"2016-12-06T10:37:00-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"title":"Feature Extraction using Word Embedding","text":"%spark.pyspark\n\nfrom pyspark.ml.feature import Word2Vec, Word2VecModel\nimport os\n\nif os.path.exists(\"/Users/ferocha/Desktop/w2v150v15w2i\"):\n    model = Word2VecModel.load(\"/Users/ferocha/Desktop/w2v150v15w2i\")\nelse:\n    word2Vec = Word2Vec(vectorSize=150, minCount=2, windowSize=15, inputCol=\"filteredWords\", outputCol=\"embeddedFeatures\",maxIter=2)\n    model = word2Vec.fit(featuredTrainDf)\nfeaturedTrainDf = model.transform(featuredTrainDf).cache()\nfeaturedTestDf = model.transform(featuredTestDf).cache()\n\nfeaturedTrainDf.registerTempTable(\"featuredTrainDf\")\nfeaturedTestDf.registerTempTable(\"featuredTestDf\")\n\nfeaturedTrainDf.show(10)","dateUpdated":"2016-12-06T11:10:23-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480453240857_350733143","id":"20161129-190040_2122893926","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+-----+--------------------+--------------------+--------------------+\n|             content|label|       filteredWords|       tfidfFeatures|    embeddedFeatures|\n+--------------------+-----+--------------------+--------------------+--------------------+\n|'The Merchant of ...|  1.0|[Merchant, Venice...|(256,[0,1,3,4,6,7...|[0.02862028432583...|\n|(Some spoilers in...|  1.0|[spoilers, includ...|(256,[0,1,2,3,4,5...|[-0.0315630293490...|\n|*!!- SPOILERS - !...|  1.0|[SPOILERS, begin,...|(256,[0,1,2,3,4,5...|[0.00856135523430...|\n|**Attention Spoil...|  1.0|[Attention, Spoil...|(256,[0,1,2,3,4,6...|[-0.0359575835374...|\n|As I expected wou...|  1.0|[expected, would,...|(256,[0,1,2,3,4,5...|[0.03414504280997...|\n|As with all of An...|  1.0|[Angelopoulos, fi...|(256,[1,2,3,4,6,8...|[0.01208713959715...|\n|Back in the mid/l...|  1.0|[Back, mid, late,...|(256,[0,1,2,4,5,6...|[-0.0429406300621...|\n|Beat a path to th...|  1.0|[Beat, path, impo...|(256,[1,4,5,6,7,8...|[0.00903130457413...|\n|Berlin-born in 19...|  1.0|[Berlin-born, 194...|(256,[0,2,3,4,5,6...|[-0.0039912504129...|\n|By 1987 Hong Kong...|  1.0|[1987, Hong, Kong...|(256,[0,1,2,3,4,6...|[-0.0052614149783...|\n+--------------------+-----+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-29T07:00:40-0200","dateStarted":"2016-12-06T11:10:23-0200","dateFinished":"2016-12-06T11:10:46-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:308"},{"text":"%spark.pyspark\n\nmodel.save(\"/Users/ferocha/Desktop/w2v100v10w2i5c\")","dateUpdated":"2016-12-06T10:30:44-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480557142645_-636562266","id":"20161130-235222_118942673","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-30T11:52:22-0200","dateStarted":"2016-12-01T06:41:51-0200","dateFinished":"2016-12-01T06:41:51-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:309"},{"text":"%spark.pyspark\n\nfeaturedTrainDf.write.mode(\"overwrite\").parquet(\"file:///Users/ferocha/Desktop/train_parquet\")\nfeaturedTestDf.write.mode(\"overwrite\").parquet(\"file:///Users/ferocha/Desktop/test_parquet\")","dateUpdated":"2016-12-06T10:30:46-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480525883149_1806191826","id":"20161130-151123_1731068425","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-30T03:11:23-0200","dateStarted":"2016-12-01T01:36:22-0200","dateFinished":"2016-12-01T01:36:30-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:310"},{"text":"%spark.pyspark\n\nfeaturedTrainDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/train_parquet\")\nfeaturedTestDf= spark.read.parquet(\"file:///Users/ferocha/Desktop/test_parquet\")","dateUpdated":"2016-12-06T10:33:48-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480528989322_-563043236","id":"20161130-160309_513202340","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-30T04:03:09-0200","dateStarted":"2016-12-06T10:33:46-0200","dateFinished":"2016-12-06T10:33:46-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"text":"%spark.pyspark\n\nfeaturedTrainDf.printSchema()\nfeaturedTrainDf.select(\"tfidfFeatures\",\"embeddedFeatures\").first()","dateUpdated":"2016-12-01T03:46:43-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480448107192_1146080880","id":"20161129-173507_1266124848","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- content: string (nullable = true)\n |-- label: double (nullable = true)\n |-- filteredWords: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- tfidfFeatures: vector (nullable = true)\n |-- embeddedFeatures: vector (nullable = true)\n\nRow(tfidfFeatures=SparseVector(100, {0: 6.0556, 1: 2.286, 2: 3.2753, 3: 2.4916, 4: 1.1326, 5: 0.7152, 6: 2.5864, 7: 0.7352, 8: 3.5355, 9: 5.3891, 10: 2.5485, 11: 5.1705, 12: 2.7246, 13: 5.2631, 14: 5.1685, 15: 2.1212, 16: 3.1395, 17: 1.7901, 18: 4.8578, 19: 3.8447, 20: 6.5006, 21: 1.8795, 22: 6.0895, 23: 3.1765, 24: 0.5901, 25: 3.8584, 26: 4.9762, 27: 1.8048, 28: 1.2758, 29: 4.2548, 30: 1.6603, 31: 5.0037, 32: 0.6964, 33: 1.1546, 34: 1.2769, 35: 1.0623, 36: 1.1646, 37: 4.2073, 38: 2.0326, 39: 4.9204, 40: 5.091, 41: 3.4627, 42: 1.8615, 43: 3.9519, 44: 1.7528, 45: 4.5618, 46: 2.6968, 47: 10.1834, 48: 2.3936, 49: 2.783, 50: 3.1947, 51: 7.8522, 52: 2.7991, 53: 6.1731, 54: 3.8788, 55: 3.287, 56: 3.0199, 57: 1.0647, 58: 4.8801, 59: 2.8572, 60: 2.0735, 61: 4.7463, 62: 3.3923, 63: 5.1481, 64: 1.3313, 65: 6.2207, 66: 5.0537, 67: 4.1511, 68: 2.3822, 69: 2.613, 70: 4.7115, 71: 6.7846, 72: 3.1042, 73: 6.0209, 74: 1.2495, 75: 2.6568, 76: 2.3498, 77: 2.3719, 78: 6.4128, 79: 3.1929, 80: 5.5564, 81: 9.5231, 82: 1.352, 83: 5.499, 84: 0.4818, 85: 3.138, 86: 3.0393, 87: 1.4207, 88: 1.9174, 89: 1.9684, 90: 2.5046, 91: 1.3136, 92: 3.2321, 93: 2.7402, 94: 1.6728, 95: 4.4687, 96: 5.2312, 97: 3.6788, 98: 4.1158, 99: 1.7705}), embeddedFeatures=DenseVector([-0.0201, -0.0297, -0.0068, -0.0467, 0.0099, 0.0288, -0.0033, 0.0055, -0.0071, -0.01, -0.0072, 0.0138, 0.0008, -0.0384, -0.0002, 0.0045, 0.0397, -0.0546, -0.0087, -0.002, 0.0413, 0.0035, 0.0436, 0.0048, -0.0379, 0.0095, -0.0043, 0.0086, 0.012, 0.0098, -0.0083, -0.0402, -0.0414, 0.0214, -0.0391, 0.0347, 0.0027, -0.0071, -0.0284, 0.0096, 0.0496, 0.0252, 0.049, -0.0006, -0.0455, -0.0822, -0.048, 0.0223, 0.0358, -0.0442, 0.0241, -0.032, -0.0159, -0.001, 0.0404, -0.0154, 0.0055, -0.0062, -0.0017, -0.0129, -0.0017, 0.0297, -0.0024, 0.0071, 0.0183, 0.0025, 0.0206, -0.0088, -0.0054, -0.0037, -0.0244, -0.017, 0.0194, 0.0291, -0.068, 0.0202, 0.0001, 0.0191, 0.0296, -0.0109, 0.0047, 0.0309, -0.0142, -0.0189, 0.0308, 0.0084, -0.0295, -0.03, 0.0319, 0.0027, 0.0118, 0.0534, 0.0252, 0.0009, 0.0379, -0.0716, -0.0598, 0.0157, -0.019, 0.0391, -0.0228, -0.0015, 0.0063, -0.0092, -0.0005, 0.0144, -0.0025, 0.0006, -0.0104, -0.0114, -0.0112, 0.017, -0.0077, 0.0265, -0.0191, 0.0092, -0.006, -0.0286, -0.0184, 0.0077, 0.0489, -0.0243, 0.0253, 0.0592, -0.0314, -0.0089, -0.044, 0.0038, 0.0181, -0.0406, -0.0228, 0.006, -0.0154, 0.0119, -0.0179, 0.0105, 0.0138, -0.0081, 0.0309, -0.0123, -0.0225, -0.0045, 0.0164, -0.0238, 0.0341, -0.0392, -0.0181, -0.0042, -0.0115, 0.0064]))\n"},"dateCreated":"2016-11-29T05:35:07-0200","dateStarted":"2016-11-30T08:52:35-0200","dateFinished":"2016-11-30T08:52:36-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:312"},{"text":"%md\n## Method 1: Decision Tree","dateUpdated":"2016-11-30T02:27:12-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480456261215_1340687359","id":"20161129-195101_2006283314","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Method 1: Decision Tree</h2>\n"},"dateCreated":"2016-11-29T07:51:01-0200","dateStarted":"2016-11-30T02:27:11-0200","dateFinished":"2016-11-30T02:27:11-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"title":"Using TFIDF","text":"%spark.pyspark\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Using TFIDF\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"tfidfFeatures\",maxDepth=10, maxBins=256,maxMemoryInMB=1024)\ndtModel = dt.fit(featuredTrainDf)\ntfidfPredictions = dtModel.transform(featuredTestDf)\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(tfidfPredictions)\nprint(\"Accuracy using TFIDF = %g \" % (accuracy))","dateUpdated":"2016-12-01T03:46:58-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480443480116_-58382915","id":"20161129-161800_1152516278","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy using TFIDF = 0.626881 \n"},"dateCreated":"2016-11-29T04:18:00-0200","dateStarted":"2016-12-01T03:15:01-0200","dateFinished":"2016-12-01T03:15:24-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:314"},{"title":"Using Word Embedding","text":"%spark.pyspark\n\n# Using Word2Vec\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"embeddedFeatures\",maxDepth=10, maxBins=128,maxMemoryInMB=1024)\ndtModel = dt.fit(featuredTrainDf)\nembeddedPredictions = dtModel.transform(featuredTestDf)\n\naccuracy = evaluator.evaluate(embeddedPredictions)\nprint(\"Accuracy using Word Embedding = %g \" % (accuracy))","dateUpdated":"2016-12-01T03:15:27-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480456206947_699787636","id":"20161129-195006_1604177065","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy using Word Embedding = 0.780518 \n"},"dateCreated":"2016-11-29T07:50:06-0200","dateStarted":"2016-12-01T03:15:27-0200","dateFinished":"2016-12-01T03:15:53-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:315"},{"text":"%md\n## Method 2: KNN","dateUpdated":"2016-11-30T12:41:53-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480456365386_599831187","id":"20161129-195245_1339193751","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Method 2: KNN</h2>\n"},"dateCreated":"2016-11-29T07:52:45-0200","dateStarted":"2016-11-30T12:41:53-0200","dateFinished":"2016-11-30T12:41:53-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:316"},{"text":"%spark.pyspark\n\nfeaturedTrainDf.registerTempTable(\"featuredTrainDf\")\nfeaturedTestDf.registerTempTable(\"featuredTestDf\")\n\n# featuredTrainDf.write.parquet(\"file:///Users/ferocha/Desktop/train_parquet\")\n# featuredTestDf.write.parquet(\"file:///Users/ferocha/Desktop/test_parquet\")","dateUpdated":"2016-12-06T10:35:45-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480461821802_-1119456835","id":"20161129-212341_987743921","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-29T09:23:41-0200","dateStarted":"2016-12-06T10:36:16-0200","dateFinished":"2016-12-06T10:37:00-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:317"},{"title":"Using TFIDF (256 features)","text":"%spark\n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.ml.Pipeline\n\nval featuredTrainDf = spark.table(\"featuredTrainDf\")\n// val featuredTrainDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/train_parquet\")\nval featuredTestDf = spark.table(\"featuredTestDf\")\n// val featuredTestDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/test_parquet\")\n\nval knn = new org.apache.spark.ml.classification.kNN_IS.kNN_ISClassifier()\n\nknn\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"tfidfFeatures\")\n    .setK(10)\n    .setDistanceType(1)\n    .setNumPartitionMap(16)\n    .setNumReduces(10)\n    .setMaxWeight(-2.0)\n    .setNumClass(2)\n    .setNumFeatures(256)\n    .setNumIter(5)\n    .setNumSamplesTest(featuredTestDf.count.toInt)\n    .setOutPath(Array(\"/Users/ferocha/Desktop\"))\n    \nval pipeline = new Pipeline().setStages(Array(knn))\nval model = pipeline.fit(featuredTrainDf)\nval predictions = model.transform(featuredTestDf)\n\n// val metrics = new MulticlassMetrics(predictions)\n// val precision = metrics.precision\n\nprintln(predictions.where(\"_1 = _2\").count().toDouble / predictions.count().toDouble)","dateUpdated":"2016-12-06T11:09:39-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480446853218_1932718882","id":"20161129-171413_1642932060","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.ml.Pipeline\n\nfeaturedTrainDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 2 more fields]\n\nfeaturedTestDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 2 more fields]\n\nknn: org.apache.spark.ml.classification.kNN_IS.kNN_ISClassifier = kNN_IS_718fc89a3b87\n\nres1: org.apache.spark.ml.classification.kNN_IS.kNN_ISClassifier = kNN_IS_718fc89a3b87\n\npipeline: org.apache.spark.ml.Pipeline = pipeline_24e5cdbdbc72\n\nmodel: org.apache.spark.ml.PipelineModel = pipeline_24e5cdbdbc72\n\npredictions: org.apache.spark.sql.DataFrame = [_1: double, _2: double]\n0.6047240802675585\n"},"dateCreated":"2016-11-29T05:14:13-0200","dateStarted":"2016-12-06T10:37:00-0200","dateFinished":"2016-12-06T11:07:56-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:318"},{"title":"Using Word Embedding","text":"import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.ml.Pipeline\n\nval featuredTrainDf = spark.table(\"featuredTrainDf\")\n// val featuredTrainDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/train_parquet\")\nval featuredTestDf = spark.table(\"featuredTestDf\")\n// val featuredTestDf = spark.read.parquet(\"file:///Users/ferocha/Desktop/test_parquet\")\n\nval knn = new org.apache.spark.ml.classification.kNN_IS.kNN_ISClassifier()\n\nknn\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"embeddedFeatures\")\n    .setK(10)\n    .setDistanceType(1)\n    .setNumPartitionMap(16)\n    .setNumReduces(10)\n    .setMaxWeight(-2.0)\n    .setNumClass(2)\n    .setNumFeatures(150)\n    .setNumIter(5)\n    .setNumSamplesTest(featuredTestDf.count.toInt)\n    .setOutPath(Array(\"/Users/ferocha/Desktop\"))\n    \nval pipeline = new Pipeline().setStages(Array(knn))\nval model = pipeline.fit(featuredTrainDf)\nval predictions = model.transform(featuredTestDf)\n\nprintln(predictions.where(\"_1 = _2\").count().toDouble / predictions.count().toDouble)","dateUpdated":"2016-12-01T03:17:15-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480455250840_1719443909","id":"20161129-193410_209088539","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.ml.Pipeline\n\nfeaturedTrainDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\nfeaturedTestDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\nknn: org.apache.spark.ml.classification.kNN_IS.kNN_ISClassifier = kNN_IS_5b62f7c065b6\n\nres4: org.apache.spark.ml.classification.kNN_IS.kNN_ISClassifier = kNN_IS_5b62f7c065b6\n\npipeline: org.apache.spark.ml.Pipeline = pipeline_683ae8f18b0a\n\nmodel: org.apache.spark.ml.PipelineModel = pipeline_683ae8f18b0a\n\npredictions: org.apache.spark.sql.DataFrame = [_1: double, _2: double]\n0.6494435612082671\n"},"dateCreated":"2016-11-29T07:34:10-0200","dateStarted":"2016-11-30T03:47:53-0200","dateFinished":"2016-11-30T03:54:38-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:319"},{"text":"%md\n## Method 3: SVM","dateUpdated":"2016-11-30T12:41:53-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480457451366_-1184129634","id":"20161129-201051_1178880008","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Method 3: SVM</h2>\n"},"dateCreated":"2016-11-29T08:10:51-0200","dateStarted":"2016-11-30T12:41:53-0200","dateFinished":"2016-11-30T12:41:53-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"title":"Using TF-IDF","text":"import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.optimization.L1Updater\n\nval featuredTrainDf = spark.table(\"featuredTrainDf\")\nval featuredTestDf = spark.table(\"featuredTestDf\")\n\nval trainData = MLUtils.convertVectorColumnsFromML(featuredTrainDf)\n    .map(row => LabeledPoint(row.getDouble(1), row(row.fieldIndex(\"tfidfFeatures\")).asInstanceOf[org.apache.spark.mllib.linalg.SparseVector])).rdd\nval testData = MLUtils.convertVectorColumnsFromML(featuredTestDf)\n    .map(row => LabeledPoint(row.getDouble(1), row(row.fieldIndex(\"tfidfFeatures\")).asInstanceOf[org.apache.spark.mllib.linalg.SparseVector])).rdd\n\n// val model = SVMWithSGD.train(trainData,20)\n\nval svmAlg = new SVMWithSGD()\nsvmAlg.optimizer\n  .setNumIterations(500)\n  .setConvergenceTol(1e-3)\n  \nval model = svmAlg.run(trainData)\n\n// model.clearThreshold()\n\nval scoreAndLabels = testData.map { point =>\n  val score = model.predict(point.features)\n  (score, point.label)\n}.toDF()\n\n// scoreAndLabels.show()\nscoreAndLabels.where(\"_1 = _2\").count().toDouble / scoreAndLabels.count().toDouble\n","dateUpdated":"2016-12-06T14:57:02-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480457477720_1985244626","id":"20161129-201117_1546395553","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.linalg.Vector\n\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n\nimport org.apache.spark.mllib.util.MLUtils\n\nimport org.apache.spark.mllib.optimization.L1Updater\n\nfeaturedTrainDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\nfeaturedTestDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\ntrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8929] at rdd at <console>:74\n\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8934] at rdd at <console>:74\n\nsvmAlg: org.apache.spark.mllib.classification.SVMWithSGD = org.apache.spark.mllib.classification.SVMWithSGD@69d956ce\n\nres32: svmAlg.optimizer.type = org.apache.spark.mllib.optimization.GradientDescent@4262e631\n\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 256, numClasses = 2, threshold = 0.0\n\nscoreAndLabels: org.apache.spark.sql.DataFrame = [_1: double, _2: double]\n\nres33: Double = 0.731814381270903\n"},"dateCreated":"2016-11-29T08:11:17-0200","dateStarted":"2016-12-06T14:57:02-0200","dateFinished":"2016-12-06T14:58:20-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:321","focus":true},{"title":"Using Word Embedding","text":"\nval trainData = MLUtils.convertVectorColumnsFromML(featuredTrainDf)\n    .map(row => LabeledPoint(row.getDouble(1), row(row.fieldIndex(\"embeddedFeatures\")).asInstanceOf[org.apache.spark.mllib.linalg.DenseVector])).rdd\nval testData = MLUtils.convertVectorColumnsFromML(featuredTestDf)\n    .map(row => LabeledPoint(row.getDouble(1), row(row.fieldIndex(\"embeddedFeatures\")).asInstanceOf[org.apache.spark.mllib.linalg.DenseVector])).rdd\n\n// val model = SVMWithSGD.train(trainData,200)\n\nval svmAlg = new SVMWithSGD()\nsvmAlg.optimizer\n  .setNumIterations(200)\n  \nval model = svmAlg.run(trainData)\n\n\n// model.clearThreshold()\n\nval scoreAndLabels = testData.map { point =>\n  val score = model.predict(point.features)\n  (score, point.label)\n}.toDF()\n\n// scoreAndLabels.show()\nscoreAndLabels.where(\"_1 = _2\").count().toDouble / scoreAndLabels.count().toDouble","dateUpdated":"2016-12-06T14:58:46-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480477839156_-1241628928","id":"20161130-015039_20958736","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[10151] at rdd at <console>:75\n\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[10156] at rdd at <console>:74\n\nsvmAlg: org.apache.spark.mllib.classification.SVMWithSGD = org.apache.spark.mllib.classification.SVMWithSGD@6cc6e9a5\n\nres36: svmAlg.optimizer.type = org.apache.spark.mllib.optimization.GradientDescent@6562225b\n\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 150, numClasses = 2, threshold = 0.0\n\nscoreAndLabels: org.apache.spark.sql.DataFrame = [_1: double, _2: double]\n\nres37: Double = 0.5948996655518395\n"},"dateCreated":"2016-11-30T01:50:39-0200","dateStarted":"2016-12-06T14:58:46-0200","dateFinished":"2016-12-06T14:59:50-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:322","focus":true},{"text":"%md\n## Method 4: Neural Network (MLP)","dateUpdated":"2016-12-06T10:33:13-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480457440664_1466790287","id":"20161129-201040_620021873","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Method 4: Neural Network (MLP)</h2>\n"},"dateCreated":"2016-11-29T08:10:40-0200","dateStarted":"2016-12-06T10:33:10-0200","dateFinished":"2016-12-06T10:33:11-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"title":"Using TF-IDF","text":"%spark.pyspark\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\n\ntrainer = MultilayerPerceptronClassifier(featuresCol=\"tfidfFeatures\",maxIter=200, layers=[4096,2], blockSize=128, seed=1234)\n\nmodel = trainer.fit(featuredTrainDf)\n\nresult = model.transform(featuredTestDf)\n    \npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))\n\n","dateUpdated":"2016-12-06T11:10:06-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480455301765_-437196270","id":"20161129-193501_1527640756","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy: 0.8434364548494984\n"},"dateCreated":"2016-11-29T07:35:01-0200","dateStarted":"2016-12-01T04:37:52-0200","dateFinished":"2016-12-01T04:38:17-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"text":"%spark.pyspark\n\nresult.show()","dateUpdated":"2016-12-06T10:31:07-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480481240160_-1050765067","id":"20161130-024720_1814421816","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+-----+--------------------+--------------------+--------------------+----------+\n|             content|label|       filteredWords|       tfidfFeatures|    embeddedFeatures|prediction|\n+--------------------+-----+--------------------+--------------------+--------------------+----------+\n|\"Buffalo Bill, He...|  1.0|[Buffalo, Bill, H...|(20,[0,1,2,3,4,5,...|[-0.0163432256989...|       1.0|\n|\"The Moon Is Blue...|  1.0|[Moon, Blue, dire...|(20,[0,1,2,3,4,5,...|[-0.0197008294100...|       1.0|\n|Wrestlemania 6, i...|  1.0|[Wrestlemania, 6,...|(20,[0,1,2,3,4,5,...|[-0.1146678446941...|       0.0|\n|In 1982, two film...|  1.0|[1982, two, films...|(20,[0,1,2,3,4,5,...|[0.00776609709922...|       1.0|\n|In Luchino Viscon...|  1.0|[Luchino, Viscont...|(20,[0,1,2,3,4,5,...|[0.04493883534556...|       1.0|\n|This is such a re...|  1.0|[revered, studied...|(20,[0,1,2,3,4,5,...|[0.00382585987897...|       1.0|\n|When Alfred Hitch...|  1.0|[Alfred, Hitchcoc...|(20,[0,1,2,3,4,5,...|[0.02854016232002...|       1.0|\n|Can a film be too...|  1.0|[film, faithful, ...|(20,[0,1,2,3,4,5,...|[0.05250687049517...|       1.0|\n|Dogtown and Z-Boy...|  1.0|[Dogtown, Z-Boys,...|(20,[0,1,2,3,4,5,...|[-0.0345528635309...|       1.0|\n|I saw 'I Smell th...|  1.0|[saw, 'I, Smell, ...|(20,[0,1,2,3,4,5,...|[-0.0086317060411...|       0.0|\n|If vampire tales ...|  1.0|[vampire, tales, ...|(20,[0,1,2,3,4,5,...|[-0.0017434583782...|       1.0|\n|What is enjoyable...|  1.0|[enjoyable, watch...|(20,[0,1,2,3,4,5,...|[-0.0028800951959...|       1.0|\n|Can you capture t...|  1.0|[capture, moment,...|(20,[0,1,2,3,4,5,...|[0.01134425179084...|       1.0|\n|There are times I...|  1.0|[times, convinced...|(20,[0,1,2,3,4,5,...|[-0.0274445053341...|       0.0|\n|First of all, des...|  1.0|[First, despite, ...|(20,[0,1,2,3,4,5,...|[-0.0496998103534...|       0.0|\n|I should no longe...|  1.0|[longer, surprise...|(20,[0,1,2,3,4,5,...|[0.03269039698295...|       1.0|\n|The 1990s was a g...|  1.0|[1990s, great, de...|(20,[0,1,2,3,4,5,...|[-0.0055246666134...|       1.0|\n|The vampire \"craz...|  1.0|[vampire, craze, ...|(20,[0,1,2,3,4,5,...|[0.01126683011844...|       1.0|\n|William Shakespea...|  1.0|[William, Shakesp...|(20,[0,1,2,3,4,5,...|[0.03607977290399...|       1.0|\n|Being a self conf...|  1.0|[self, confessed,...|(20,[0,1,2,3,4,5,...|[-0.0100942788478...|       1.0|\n+--------------------+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"},"dateCreated":"2016-11-30T02:47:20-0200","dateStarted":"2016-11-30T02:47:44-0200","dateFinished":"2016-11-30T02:47:44-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:325"},{"title":"Using Word Embedding","text":"%spark.pyspark\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\ntrainer = MultilayerPerceptronClassifier(featuresCol=\"embeddedFeatures\",maxIter=400, layers=[150,2], blockSize=128, seed=1234, tol=1e-7)\n\nmodel = trainer.fit(featuredTrainDf)\n\nresult = model.transform(featuredTestDf)\n\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))","dateUpdated":"2016-12-06T11:16:31-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480480609905_2043096073","id":"20161130-023649_1179308799","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy: 0.8835702341137124\n"},"dateCreated":"2016-11-30T02:36:49-0200","dateStarted":"2016-12-06T11:10:42-0200","dateFinished":"2016-12-06T11:11:04-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"text":"%spark.pyspark\n\nfrom pyspark.sql.functions import *\n\nresult.registerTempTable(\"result\")\n\nz.show(spark.sql(\"select label, prediction, count(*) from result group by label, prediction\"))","dateUpdated":"2016-12-06T13:55:35-0200","config":{"colWidth":12,"graph":{"mode":"table","height":179,"optionOpen":true,"keys":[{"name":"label","index":0,"aggr":"sum"},{"name":"prediction","index":1,"aggr":"sum"}],"values":[{"name":"count(1)","index":2,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"prediction","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481038287022_-1280741212","id":"20161206-133127_399512224","result":{"code":"SUCCESS","type":"TABLE","msg":"label\tprediction\tcount(1)\n1.0\t1.0\t2105\n0.0\t1.0\t270\n1.0\t0.0\t287\n0.0\t0.0\t2122\n\n","comment":"","msgTable":[[{"key":"prediction","value":"1.0"},{"key":"prediction","value":"1.0"},{"key":"prediction","value":"2105"}],[{"key":"count(1)","value":"0.0"},{"key":"count(1)","value":"1.0"},{"key":"count(1)","value":"270"}],[{"value":"1.0"},{"value":"0.0"},{"value":"287"}],[{"value":"0.0"},{"value":"0.0"},{"value":"2122"}]],"columnNames":[{"name":"label","index":0,"aggr":"sum"},{"name":"prediction","index":1,"aggr":"sum"},{"name":"count(1)","index":2,"aggr":"sum"}],"rows":[["1.0","1.0","2105"],["0.0","1.0","270"],["1.0","0.0","287"],["0.0","0.0","2122"]]},"dateCreated":"2016-12-06T13:31:27-0200","dateStarted":"2016-12-06T13:33:51-0200","dateFinished":"2016-12-06T13:33:54-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:327"},{"text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.ml.linalg._\n\nval vectorCat = (x:org.apache.spark.ml.linalg.Vector, y: org.apache.spark.ml.linalg.Vector) => {\n    val arr = x.toDense.toArray ++ y.toDense.toArray\n    org.apache.spark.ml.linalg.Vectors.dense(arr).toSparse\n}\nval vectorCatUdf = udf(vectorCat)\n\nval featuredTrainDf = spark.table(\"featuredTrainDf\")\nval featuredTestDf = spark.table(\"featuredTestDf\")\n\nfeaturedTrainDf.withColumn(\"combinedFeatures\",vectorCatUdf($\"tfidfFeatures\",$\"embeddedFeatures\")).cache.registerTempTable(\"combinedTrainDf\")\nfeaturedTestDf.withColumn(\"combinedFeatures\",vectorCatUdf($\"tfidfFeatures\",$\"embeddedFeatures\")).cache.registerTempTable(\"combinedTestDf\")\n","dateUpdated":"2016-12-01T11:01:10-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480572346555_-478720131","id":"20161201-040546_754037583","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.ml.linalg._\n\nvectorCat: (org.apache.spark.ml.linalg.Vector, org.apache.spark.ml.linalg.Vector) => org.apache.spark.ml.linalg.SparseVector = <function2>\n\nvectorCatUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7, org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n\nfeaturedTrainDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\nfeaturedTestDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"},"dateCreated":"2016-12-01T04:05:46-0200","dateStarted":"2016-12-01T11:01:11-0200","dateFinished":"2016-12-01T11:01:13-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"text":"\n\nval trainer = new org.apache.spark.ml.scaladl.MultilayerPerceptronClassifier()\n    .setFeaturesCol(\"embeddedFeatures\")\n    .setPredictionCol(\"prediction\")\n    .setLayers(Array(150, 100, 2)).setMaxIter(300).setTol(1e-9)\n\nval featuredTrainDf = spark.table(\"featuredTrainDf\")\nval featuredTestDf = spark.table(\"featuredTestDf\")\n\nval model = trainer.fit(featuredTrainDf)\nval result = model.transform(featuredTestDf)\n\nresult.where(\"label = prediction\").count().toDouble / result.count().toDouble","dateUpdated":"2016-12-06T13:38:41-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480771216046_642312391","id":"20161203-112016_243882921","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntrainer: org.apache.spark.ml.scaladl.MultilayerPerceptronClassifier = mlpc-scaladl_5ae08f942dc1\n\nfeaturedTrainDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 2 more fields]\n\nfeaturedTestDf: org.apache.spark.sql.DataFrame = [content: string, label: double ... 2 more fields]\n\nmodel: org.apache.spark.ml.scaladl.MultilayerPerceptronClassificationModel = mlpc-scaladl_5ae08f942dc1\n\nresult: org.apache.spark.sql.DataFrame = [content: string, label: double ... 3 more fields]\n\nres32: Double = 0.8875418060200669\n"},"dateCreated":"2016-12-03T11:20:16-0200","dateStarted":"2016-12-03T12:05:54-0200","dateFinished":"2016-12-03T12:07:13-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"title":"Combining both","text":"%spark.pyspark\n\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\ntrainer = MultilayerPerceptronClassifier(featuresCol=\"combinedFeatures\",maxIter=400, layers=[4096+150,100,50,50,30,30,20,10,2], blockSize=128, seed=1234,tol=1e-7)\n\nmodel = trainer.fit(spark.table(\"combinedTrainDf\"))\n\nresult = model.transform(spark.table(\"combinedTestDf\"))\n\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))","dateUpdated":"2016-12-06T11:14:58-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480572167249_-1420245017","id":"20161201-040247_1449848749","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy: 0.8273411371237458\n"},"dateCreated":"2016-12-01T04:02:47-0200","dateStarted":"2016-12-01T11:24:37-0200","dateFinished":"2016-12-01T11:46:36-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"text":"%spark.pyspark\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlr = LogisticRegression(maxIter=100, regParam=0.0, featuresCol=\"embeddedFeatures\", labelCol=\"label\")\n\nmodel = lr.fit(featuredTrainDf)\n\nresult = model.transform(featuredTestDf)\n\npredictionAndLabels = result.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))","dateUpdated":"2016-12-01T01:13:39-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480485890581_-324513506","id":"20161130-040450_508596219","result":{"code":"SUCCESS","type":"TEXT","msg":"Accuracy: 0.8846153846153846\n"},"dateCreated":"2016-11-30T04:04:50-0200","dateStarted":"2016-12-01T01:13:39-0200","dateFinished":"2016-12-01T01:13:45-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"title":"Cross-validation for MLP using Word Embedding","text":"%spark.pyspark\n\ndf = featuredTrainDf.union(featuredTestDf)\n\nsplits = df.randomSplit([0.1]*10, seed=1234)\n\nfor split in splits:\n    split.cache()\n\nfor i in range(10):\n    ls = splits[:]\n    \n    test = ls[i]\n    del ls[i]\n    \n    train = ls[0]\n    for j in range(1,9):\n        train = train.union(ls[j])\n        \n    print(\"Fold %d\" % (i+1))\n    \n    trainer = MultilayerPerceptronClassifier(featuresCol=\"embeddedFeatures\",maxIter=100, layers=[150,2], blockSize=128, seed=1234)\n\n    model = trainer.fit(train)\n    \n    result = model.transform(test)\n    \n    predictionAndLabels = result.select(\"prediction\", \"label\")\n    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n    print(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))\n    \n    \n    ","dateUpdated":"2016-12-06T11:15:14-0200","config":{"colWidth":12,"graph":{"mode":"table","height":324,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480574912106_1205864614","id":"20161201-044832_882233307","result":{"code":"SUCCESS","type":"TEXT","msg":"Fold 1\nAccuracy: 0.8817704122877931\nFold 2\nAccuracy: 0.8899702085402185\nFold 3\nAccuracy: 0.883356476889506\nFold 4\nAccuracy: 0.8818573667711599\nFold 5\nAccuracy: 0.8861182014529747\nFold 6\nAccuracy: 0.8740786240786241\nFold 7\nAccuracy: 0.8794081880826915\nFold 8\nAccuracy: 0.8775224775224775\nFold 9\nAccuracy: 0.8820633983067533\nFold 10\nAccuracy: 0.8874307976214886\n"},"dateCreated":"2016-12-01T04:48:32-0200","dateStarted":"2016-12-01T04:54:00-0200","dateFinished":"2016-12-01T05:21:58-0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"text":"","dateUpdated":"2016-12-01T03:49:23-0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480614552862_-3033767","id":"20161201-154912_854411294","dateCreated":"2016-12-01T03:49:12-0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:333"}],"name":"T3 IA","id":"2C2TCUC86","angularObjects":{"2C3WFZQ3N:shared_process":[],"2C2QSUAM5:shared_process":[],"2C2QGAGQH:shared_process":[],"2BZZ6RF4U:shared_process":[],"2C36M674G:shared_process":[],"2C3M6KSHR:shared_process":[],"2BZS17UYF:shared_process":[],"2BZM8STWX:shared_process":[],"2C2NWB617:shared_process":[],"2BZPH9M7Q:shared_process":[],"2C2CQ51PA:shared_process":[],"2BZKW64M9:shared_process":[],"2BZJ7SM78:shared_process":[],"2BZUAAUX4:shared_process":[],"2C2HGYSWU:shared_process":[],"2C27P529M:shared_process":[],"2BZX19KJ9:shared_process":[],"2C2GRU8ZU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}